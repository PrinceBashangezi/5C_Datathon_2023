{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neseccary modules.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datathon_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin preliminary analysis of the data. Exploratory phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Id column\n",
    "train_data.drop(\"Id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have an imbalanced dataset? Yes...yes we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have an imbalanced dataset? Let's find the distribution of the target variable\n",
    "# Notice how there are many more flights (rows) that aren't delayed than flights that are\n",
    "pd.value_counts(train_data[\"IS_DELAYED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide what columns/features to use in our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Are any the features basically the \"same\" as another feature? Well, the airports and cities say the same thing, DEP_DEL_NEW should be removed, and manufacture year is the same variable as the age of the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop([\"ORIGIN_CITY_NAME\", \"DEST_CITY_NAME\", \"MANUFACTURE_YEAR\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to turn the categorical variales into numbers so we can use them to train our model! We'll use sklearn's `LabelEncoder()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how a label encoder works: \n",
    "\n",
    "`fit`\n",
    "If we have a list of discrete variables, like [\"a\", \"b\", \"b\", \"c\"], the label encoder will locate each unique item in the list (\"a\", \"b\", \"c\") and assign an integer to that object, for instance, \n",
    "\n",
    "\"a\" -> 0\n",
    "\n",
    "\"b\" -> 1\n",
    "\n",
    "\"c\" -> 2\n",
    "\n",
    "`transform`\n",
    "Now, when we encounter a list like [\"b\", \"b\", \"c\", \"a\"], the LabelENcoder will perform the translation between string and number, and output [1, 1, 2, 0]. Essentially, replacing the string with the corresponding number.\n",
    "\n",
    "Label encoders, however, do not handle unseen values. So, if we try to translate \"d\", the LabelEncoder will through a bad error. If out training set contains only \"a\", \"b\", and \"c\", and our testing set contains a new string \"d\", we'll run into a problem. To account for this, we'll add an \"UNSEEN\" to the unique items in the list, so when we encounter an unknown value in the testing set, we'll replace it with \"UNSEEN\" and continue encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the string, categorical variables, we must encode these values as numbers. \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dest_le = LabelEncoder().fit(train_data[\"DEST\"].tolist() + [\"UNSEEN\"])\n",
    "train_data[\"DEST\"] = dest_le.transform(train_data[\"DEST\"])\n",
    "\n",
    "carrier_name_le = LabelEncoder().fit(train_data[\"CARRIER_NAME\"].tolist()+ [\"UNSEEN\"])\n",
    "train_data[\"CARRIER_NAME\"] = carrier_name_le.transform(train_data[\"CARRIER_NAME\"])\n",
    "\n",
    "previous_airport_le = LabelEncoder().fit(train_data[\"PREVIOUS_AIRPORT\"].tolist()+ [\"UNSEEN\"])\n",
    "train_data[\"PREVIOUS_AIRPORT\"] = previous_airport_le.transform(train_data[\"PREVIOUS_AIRPORT\"])\n",
    "\n",
    "dep_time_blk_le = LabelEncoder().fit(train_data[\"DEP_TIME_BLK\"].tolist()+ [\"UNSEEN\"])\n",
    "train_data[\"DEP_TIME_BLK\"] = dep_time_blk_le.transform(train_data[\"DEP_TIME_BLK\"])\n",
    "\n",
    "departing_airport_le = LabelEncoder().fit(train_data[\"DEPARTING_AIRPORT\"].tolist()+ [\"UNSEEN\"])\n",
    "train_data[\"DEPARTING_AIRPORT\"] = departing_airport_le.transform(train_data[\"DEPARTING_AIRPORT\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For this starter code, I'll select 5 random variables to use as my features. You should do your own selection, and think about what features would be useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['DEPARTING_AIRPORT', 'DEST', 'PLANE_AGE', 'CARGO_HANDLING', 'PRCP', 'AWND', 'GROUND_SERV_PER_PASS', 'PREVIOUS_AIRPORT', \"DEP_TIME_BLK\", \"IS_DELAYED\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert this dataframe into a numpy array to begin the model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np = train_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we separate the features from the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data_np[:, :-1] # All rows, and every column except for the last one, which is the target variable\n",
    "y = train_data_np[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the data into a training set and testing set so we can both train the model, and evaluate the model after training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# IF YOUR MODEL IS TAKING TOO LONG TO RUN, INCREASE THE TEST SIZE to 0.7 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our model. This is truly where the magic happens, and it's truly just plug and play. Feel free to swap out my model with any one of these, and explore how the results change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I'm just defining a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GaussianNB()\n",
    "model = StandardScaler(\n",
    "n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that our model is done training, let's see how we did. To evaluate our model we must define what metric we evaluate our model on. We'll be using AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our testing subset and make predictions\n",
    "y_test_predictions_probabilities = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_proba` is a function that returns the probability/confidence of the model for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_predictions_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we exaime the first row [0.57795736, 0.42204264], we interpret this as the model is 57% confident that the label should be 0, and 42% confident that the label should be 1. The AUROC Score is concerned only with the probability of the 1 label, so we must grab the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = y_test_predictions_probabilities[:, 1] # All rows, second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is decent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our test data and make predictions on that, then create our submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"datathon_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used only the \"SEGMENT_NUMBER\", \"NUMBER_OF_SEATS\", \"PRCP\", \"CARGO_HANDLING\", \"AIRLINE_FLIGHTS_MONTH\", \"DEP_TIME_BLK\" columns when training, we must only use these when testing, because these features are what our model is trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we MUST keep the Id column here to create our submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[[\"Id\", 'DEPARTING_AIRPORT', 'DEST', 'PLANE_AGE', 'CARGO_HANDLING', 'PRCP', 'AWND', 'GROUND_SERV_PER_PASS', 'PREVIOUS_AIRPORT', \"DEP_TIME_BLK\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me must make the DEPARTING_AIRPORT, CARRIER_NAME, and  DEP_TIME_BLK. numerical using the SAME label encoder we used on our train data for consistency, but first, as mentioned before, we must check if there are any values in these categories that weren't in the training data so we don't run into any errors. If we find any, we replace them with \"UNSEEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dep_time_blk = []\n",
    "for value in test_data[\"DEP_TIME_BLK\"]:\n",
    "       # If the value is unknown, we tag the \"UNSEEN\"\n",
    "       if value not in dep_time_blk_le.classes_:\n",
    "              new_dep_time_blk.append(\"UNSEEN\")\n",
    "       # If the value is known to the labelencoder, we can safely append that value\n",
    "       else:\n",
    "              new_dep_time_blk.append(value)\n",
    "# Replace\n",
    "test_data[\"DEP_TIME_BLK\"] = new_dep_time_blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_departing_airport = []\n",
    "for value in test_data[\"DEPARTING_AIRPORT\"]:\n",
    "       # If the value is unknown, we tag the \"UNSEEN\"\n",
    "       if value not in departing_airport_le.classes_:\n",
    "              new_departing_airport.append(\"UNSEEN\")\n",
    "       # If the value is known to the labelencoder, we can safely append that value\n",
    "       else:\n",
    "              new_departing_airport.append(value)\n",
    "# Replace\n",
    "test_data[\"DEPARTING_AIRPORT\"] = new_departing_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dest = []\n",
    "for value in test_data[\"DEST\"]:\n",
    "       # If the value is unknown, we tag the \"UNSEEN\"\n",
    "       if value not in dest_le.classes_:\n",
    "              new_dest.append(\"UNSEEN\")\n",
    "       # If the value is known to the labelencoder, we can safely append that value\n",
    "       else:\n",
    "              new_dest.append(value)\n",
    "# Replace\n",
    "test_data[\"DEST\"] = new_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_previous_airport = []\n",
    "for value in test_data[\"PREVIOUS_AIRPORT\"]:\n",
    "       # If the value is unknown, we tag the \"UNSEEN\"\n",
    "       if value not in previous_airport_le.classes_:\n",
    "              new_previous_airport.append(\"UNSEEN\")\n",
    "       # If the value is known to the labelencoder, we can safely append that value\n",
    "       else:\n",
    "              new_previous_airport.append(value)\n",
    "# Replace\n",
    "test_data[\"PREVIOUS_AIRPORT\"] = new_previous_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"DEP_TIME_BLK\"] = dep_time_blk_le.transform(test_data[\"DEP_TIME_BLK\"])\n",
    "test_data[\"DEPARTING_AIRPORT\"] = departing_airport_le.transform(test_data[\"DEPARTING_AIRPORT\"])\n",
    "test_data[\"DEST\"] = dest_le.transform(test_data[\"DEST\"])\n",
    "test_data[\"PREVIOUS_AIRPORT\"] = previous_airport_le.transform(test_data[\"PREVIOUS_AIRPORT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's now do the same thing as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now entirely test data, and we don't need to split using `train_test_split` because we're not training a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST = test_data_np[:, 1:] # The first column is the Id column, which we do not want to keep in our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(X_TEST) # Just like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to make our submission file! The submission file has two columns to named exactly this way. \"Id\", and \"IS_DELAYED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_data[[\"Id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"IS_DELAYED\"] = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the dataframe into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"test_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
